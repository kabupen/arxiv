## 3D Question Answering
3D質問応答 

- http://arxiv.org/pdf/2112.08359v1
### summary
視覚的な質問応答（VQA）は、近年、目覚ましい進歩を遂げています。ただし、ほとんどの取り組みは、2D画像の質問回答タスクにのみ焦点を当てています。このホワイトペーパーでは、VQAを3Dドメインに拡張する最初の試みを紹介します。これにより、人工知能による3D実世界のシナリオの認識が容易になります。画像ベースのVQAとは異なり、3D質問応答（3DQA）は、入力としてカラーポイントクラウドを使用し、3D関連の質問に回答するために外観と3Dジオメトリ理解能力の両方を必要とします。この目的のために、外観とジオメトリ情報をそれぞれ活用するための2つのエンコーダで構成される新しいトランスベースの3DQAフレームワーク\ textbf {`` 3DQA-TR "}を提案します。外観、ジオメトリ、および言語のマルチモーダル情報質問は最終的にa3D-LinguisticBertを介して相互に対応し、ターゲットの回答を予測できます。提案された3DQAフレームワークの有効性を検証するために、ScanNetデータセットと$ \ sim $ 6Kquestions、$ 806 $シーンに対する$ \ sim $ 30Kの回答が含まれています。このデータセットに関する広範な実験は、提案された3DQAフレームワークが既存のVQAフレームワークよりも明らかに優れていること、および主要な設計の有効性を示しています。私たちのコードとデータセットは、この方向での研究を容易にするために公開されます。
--------------------------------------------------


## Reliable Multi-Object Tracking in the Presence of Unreliable Detections
信頼性の低い検出が存在する場合の信頼性の高いマルチオブジェクト追跡 

- http://arxiv.org/pdf/2112.08345v1
### summary
最近のマルチオブジェクト追跡（MOT）システムは、非常に正確なオブジェクト検出器を活用しています。ただし、このような検出器のトレーニングには、大量のラベル付きデータが必要です。このようなデータは人間や乗り物で広く利用できますが、他の動物種ではかなり不足しています。検出品質が低い場合でも堅牢なパフォーマンスを維持するように設計されたアルゴリズムであるRobustConfidenceTracking（RCT）を紹介します。検出信頼性情報を破棄する以前の方法とは対照的に、RCTは根本的に異なるアプローチを採用し、正確な検出信頼性値に依存してトラック、拡張トラック、およびフィルタートラックを初期化します。特に、RCTは、信頼性の低い検出を（単一オブジェクトトラッカーとともに）効率的に使用してオブジェクトを継続的に追跡することにより、IDスイッチを最小限に抑えることができます。信頼性の低い検出が存在する場合にトラッカーを評価するために、挑戦的な実世界の水中魚追跡データセットFISHTRACを紹介します。 FISHTRACおよびUA-DETRACデータセットの評価では、最先端のディープシングルおよびマルチオブジェクトトラッカーやより古典的なアプローチなど、不完全な検出が提供された場合、RCTが他のアルゴリズムよりも優れていることがわかりました。具体的には、RCThasは、すべてのシーケンスの結果を正常に返すメソッド全体で最高の平均HOTAであり、他のメソッドよりもIDスイッチが大幅に少なくなっています。
--------------------------------------------------


## ForgeryNet -- Face Forgery Analysis Challenge 2021: Methods and Results
ForgeryNet-Face Forgery Analysis Challenge 2021：方法と結果 

- http://arxiv.org/pdf/2112.08325v1
### summary
フォトリアリスティックな合成技術の急速な進歩は、実際の画像と操作された画像の境界がぼやけ始める臨界点に達しています。最近、290万枚の画像と221,247本の動画からなるメガスケールの深顔偽造データセットForgeryNetがリリースされました。これは、データスケール、操作（7つの画像レベルのアプローチ、8つのビデオレベルのアプローチ）、摂動（36の独立した、より混合された摂動）、および注釈（630万の分類ラベル、290万の操作領域の注釈、および221,247の一時的な偽造セグメントラベル）。このホワイトペーパーでは、ForgeryNetbenchmarkを採用したTheForgeryNet-Face Forgery Analysis Challenge2021の方法と結果について報告します。モデルの評価は、プライベートテストセットでオフラインで実行されます。合計186名の参加者がコンテストに登録し、11チームが有効な提出を行いました。トップランクのソリューションを分析し、今後の作業の方向性についていくつかの議論を提示します。
--------------------------------------------------


## Detecting Object States vs Detecting Objects: A New Dataset and a Quantitative Experimental Study
オブジェクトの状態の検出とオブジェクトの検出：新しいデータセットと定量的実験研究 

- http://arxiv.org/pdf/2112.08281v1
### summary
画像内のオブジェクトの状態の検出（状態検出-SD）は、理論的にも実際的にも重要な問題であり、アクション認識やアフォーダンス検出などの他の重要なコンピュータービジョンの問題と緊密に絡み合っています。また、ロボットシステムやインテリジェントエージェントなど、動的ドメインで推論して行動する必要のあるエンティティにも非常に関連性があります。その重要性にもかかわらず、これまで、この問題に関する研究は限られていました。この論文では、SD問題の体系的な研究を試みます。最初に、オブジェクト状態検出データセット（OSDD）を紹介します。これは、18のオブジェクトカテゴリと9つの状態クラスの19,000を超える注釈で構成される新しい公開データセットです。次に、オブジェクト検出（OD）に使用される標準の深層学習フレームワークを使用して、SD問題の動作の詳細な調査に向けて、適切に設計された多数の実験を実施します。この調査により、さまざまなシナリオで、SDのパフォーマンス、およびODと比較した相対的なパフォーマンスに関するベースラインを設定できます。全体として、実験結果は、SDがODよりも困難であり、この重大な問題に効果的に対処するために調整されたSDメソッドを開発する必要があることを確認しています。
--------------------------------------------------


## SeqFormer: a Frustratingly Simple Model for Video Instance Segmentation
SeqFormer：ビデオインスタンスセグメンテーションのためのイライラするほど単純なモデル 

- http://arxiv.org/pdf/2112.08275v1
### summary
この作業では、ビデオインスタンスセグメンテーションのイライラするほど単純なモデルであるSeqFormerを紹介します。 SeqFormerは、ビデオフレーム間のインスタンス関係をモデル化するビジョントランスフォーマーの原理に従います。それでも、ビデオ内のインスタンスの時系列をキャプチャするには、スタンドアロンのインスタンスクエリで十分であることがわかりますが、注意メカニズムは各フレームで独立して実行する必要があります。これを実現するために、SeqFormerは各フレームでインスタンスを特定し、時間情報を集約して、各フレームのマスクシーケンスを動的に予測するために使用されるビデオレベルのインスタンスの強力な表現を学習します。インスタンスの追跡は、ブランチや後処理を追跡することなく自然に実現されます。 YouTube-VISデータセットでは、SeqFormerはResNet-50バックボーンで47.4 APを達成し、ベルやホイッスルのないResNet-101バックボーンで49.0APを達成します。このような成果は、以前の最先端のパフォーマンスをそれぞれ4.6および4.4大幅に上回っています。さらに、最近提案されたSwinトランスフォーマーと統合されたSeqFormerは、59.3というはるかに高いAPを実現します。 SeqFormerが、ビデオインスタンスのセグメンテーションにおける将来の研究を促進する強力なベースラインになることを願っています。その間、この分野をより堅牢で正確な、きちんとしたモデルで前進させます。コードと事前トレーニング済みモデルは、https：//github.com/wjf5203/SeqFormerで公開されています。
--------------------------------------------------


## Putting People in their Place: Monocular Regression of 3D People in Depth
人々をその場に置く：3D人々の深さの単眼回帰 

- http://arxiv.org/pdf/2112.08274v1
### summary
複数の人がいるイメージを前提として、私たちの目標は、すべての人のポーズと形状、および相対的な深さを直接回帰することです。ただし、画像内の人物の奥行きを推測することは、身長を知らなくても基本的にあいまいです。これは、シーンに非常に異なるサイズの人が含まれている場合に特に問題になります。幼児から大人まで。これを解決するには、いくつかのことが必要です。まず、1つの画像で複数の人物のポーズと奥行きを推測する新しい方法を開発します。複数の人を推定する以前の作業では、画像平面で推論することでこれを行いますが、BEVと呼ばれるこの方法では、奥行きについて明示的に推論するために、架空の鳥瞰図表現を追加します。 BEVは、画像内の体の中心と奥行きについて同時に推論し、これらを組み合わせることにより、3Dの体の位置を推定します。以前の作業とは異なり、BEVはエンドツーエンドで差別化できるシングルショット方式です。第二に、高さは年齢によって変化するため、画像内の人物の年齢も推定せずに深度を解決することは不可能です。そのために、BEVが乳幼児から成人までの形状を推測できる3Dボディモデルスペースを活用します。第三に、BEVをトレーニングするには、新しいデータセットが必要です。具体的には、年齢ラベルと画像内の人物間の相対的な深さの関係を含む「Relative Human」（RH）データセットを作成します。 RHとAGORAに関する広範な実験は、モデルとトレーニングスキームの有効性を示しています。 BEVは、深さの推論、子の形状の推定、および閉塞に対するロバスト性に関して、既存の方法よりも優れています。コードとデータセットは、研究目的でリリースされます。
--------------------------------------------------


## RA V-Net: Deep learning network for automated liver segmentation
RA V-Net：自動肝臓セグメンテーションのためのディープラーニングネットワーク 

- http://arxiv.org/pdf/2112.08232v1
### summary
肝臓の正確なセグメンテーションは、病気の診断の前提条件です。自動セグメンテーションは、コンピューターを利用した肝疾患の検出と診断の重要なアプリケーションです。近年、医用画像の自動処理は飛躍的な進歩を遂げています。ただし、腹部スキャンCT画像のコントラストが低く、肝臓の形態が複雑なため、正確な自動セグメンテーションが困難です。本論文では、U-Netに基づく改良された医用画像自動セグメンテーションモデルであるRAV-Netを提案する。それは次の3つの主要な革新を持っています。 CofResモジュール（Composite OriginalFeature Residual Module）が提案されています。より複雑な畳み込みレイヤーとスキップ接続を使用して、より高いレベルの画像特徴抽出機能を取得し、勾配の消失や爆発を防ぎます。モデルの計算量を減らすために、ARモジュール（注意回復モジュール）が提案されています。さらに、エンコーディングモジュールとデコーディングモジュールのデータピクセル間の空間的特徴は、チャネルとLSTMコンボリューションを調整することによって検出されます。最後に、画像の特徴は効果的に保持されます。 CA Module（Channel Attention Module）が導入されました。これは、依存関係のある関連チャネルを抽出し、マトリックスドット積によってそれらを強化する一方で、依存関係のない無関係なチャネルを弱めるために使用されます。チャネルアテンションの目的は達成されます。 LSTM畳み込みとCAモジュールによって提供される注意メカニズムは、ニューラルネットワークのパフォーマンスを強力に保証します。 U-Netネットワークの精度：0.9862、精度：0.9118、DSC：0.8547、JSC：0.82。RAV-Netの評価指標、精度：0.9968、精度：0.9597、DSC：0.9654、JSC：0.9414。セグメンテーション効果の最も代表的なメトリックはDSCであり、U-Netよりも0.1107向上し、JSCは0.1214を向上させます。
--------------------------------------------------


## An Experimental Study of the Impact of Pre-training on the Pruning of a Convolutional Neural Network
畳み込みニューラルネットワークの剪定に対する事前トレーニングの影響の実験的研究 

- http://arxiv.org/pdf/2112.08227v1
### summary
近年、ディープニューラルネットワークはさまざまなアプリケーションドメインで幅広い成功を収めています。ただし、重要な計算リソースとメモリリソースが必要であるため、特にモバイルデバイスやリアルタイムアプリケーションでの展開が大幅に妨げられます。ニューラルネットワークは通常、ネットワークの重みに対応する多数のパラメーターを含みます。トレーニングプロセスの助けを借りて取得されたこのようなパラメータは、ネットワークのパフォーマンスを決定します。ただし、それらは非常に冗長でもあります。剪定方法は、特に、無関係な重みを識別して削除することにより、パラメーターセットのサイズを縮小しようとします。この論文では、剪定効率に対するトレーニング戦略の影響を調べます。 4つのデータセット（CIFAR10、CIFAR100、SVHN、Caltech101）と2つの異なるCNN（VGG16とMobileNet）で得られた実験結果は、大規模なコーパス（ImageNetなど）で事前トレーニングされ、特定のデータセットで微調整されると、最初からトレーニングされた同じネットワークよりもはるかに効率的に（パラメーター削減の最大80％）プルーニングできます。
--------------------------------------------------


## Quantitative analysis of visual representation of sign elements in COVID-19 context
COVID-19コンテキストでのサイン要素の視覚的表現の定量分析 

- http://arxiv.org/pdf/2112.08219v1
### summary
表現とは、人間が外部と内部の両方で起こっていることの現実を表現する方法です。したがって、コミュニケーションの手段としての視覚的表現は、話し言葉や書き言葉と同じように、要素を使用して物語を構築します。コンピューター分析を使用して、エピデミックに関連して作成された視覚的作品に使用されている要素の定量分析を実行することを提案します。TheCovidArtMuseumのInstagramアカウントにコンパイルされた画像を使用して、グローバルに関する主観的な体験を表すために使用されるさまざまな要素を分析します。イベント。このプロセスは、アルゴリズムが各研究画像に含まれるオブジェクトを学習および検出できるように、画像内のオブジェクトを検出するための機械学習に基づく手法で実行されています。この調査は、サンプルで確立された物語と関連性の関係を作成するために画像で繰り返される要素を明らかにし、すべての作成に伴う主観性にもかかわらず、オブジェクトの選択に関しては、共有および削減された決定の特定のパラメーターがあると結論付けています視覚的表現に含まれる
--------------------------------------------------


## Multi-View Depth Estimation by Fusing Single-View Depth Probability with Multi-View Geometry
シングルビュー深度確率とマルチビュージオメトリの融合によるマルチビュー深度推定 

- http://arxiv.org/pdf/2112.08177v1
### summary
マルチビュー深度推定方法では、通常、マルチビューコストボリュームの計算が必要になります。これにより、大量のメモリが消費され、推論が遅くなります。さらに、マルチビューマッチングは、テクスチャのないサーフェス、反射サーフェス、および移動するオブジェクトでは失敗する可能性があります。このような故障モードの場合、単一ビュー深度の推定方法の方が信頼性が高いことがよくあります。この目的のために、マルチビュー深度推定の精度、堅牢性、および効率を向上させるために、シングルビュー深度確率をマルチビュージオメトリと融合するための新しいフレームワークであるMaGNetを提案します。 MaGNetは、フレームごとに、ピクセル単位のガウス分布としてパラメーター化された単一ビュー深度の確率分布を推定します。次に、参照フレームに対して推定された分布を使用して、ピクセルごとの深さの候補をサンプリングします。このような確率的サンプリングにより、ネットワークは、より少ない深度候補を評価しながら、より高い精度を達成できます。また、マルチビューの深度がシングルビューの予測と一致することを保証するために、マルチビューのマッチングスコアの深度整合性の重み付けを提案します。提案された方法は、ScanNet、7-Scenes、およびKITTIで最先端のパフォーマンスを実現します。定性的評価は、この方法が、テクスチャのない/反射する表面や移動するオブジェクトなどの挑戦的なアーティファクトに対してより堅牢であることを示しています。
--------------------------------------------------


## Improving Self-supervised Learning with Automated Unsupervised Outlier Arbitration
自動化された教師なし外れ値アービトレーションによる自己監視学習の改善 

- http://arxiv.org/pdf/2112.08132v1
### summary
私たちの仕事は、既存の主流の自己教師あり学習方法の構造化された欠点を明らかにしています。自己監視型学習フレームワークは、通常、一般的な完全なインスタンスレベルの不変性の仮説を認めていますが、背後にある落とし穴を注意深く調査します。特に、複数のポジティブビューを生成するための既存の拡張パイプラインは、ダウンストリームタスクの学習を損なう分布外（OOD）サンプルを自然に導入すると主張します。入力に多様な前向きな増強を生成することは、下流のタスクに利益をもたらすことで常に報われるとは限りません。この固有の欠陥を克服するために、自己監視学習のビューサンプリングの問題を対象とした軽量潜在変数modelUOTAを導入します。 UOTAは、ビューを生成するために最も重要なサンプリング領域を適応的に検索し、外れ値にロバストな自己監視学習アプローチの実行可能な選択肢を提供します。私たちの方法は、損失の性質が対照的であるかどうかに関係なく、多くの主流の自己監視学習アプローチに直接一般化されます。明らかなマージンを備えた最先端の自己監視パラダイムに対するUOTAの利点を経験的に示しています。これは、既存のアプローチに埋め込まれたOODsampleの問題の存在を正当化するものです。特に、提案のメリットは、推定値の分散とバイアスの削減が保証されることを理論的に証明します。コードはhttps://github.com/ssl-codelab/uotaで入手できます。
--------------------------------------------------


## Self-Supervised Monocular Depth and Ego-Motion Estimation in Endoscopy: Appearance Flow to the Rescue
内視鏡検査における自己監視単眼深度と自我運動推定：救助への外観の流れ 

- http://arxiv.org/pdf/2112.08122v1
### summary
最近、自己監視学習技術が単眼ビデオから深度と自我運動を計算するために適用され、自律運転シナリオで驚くべきパフォーマンスを達成しています。奥行きと自我運動の自己教師あり学習の広く採用されている仮定の1つは、画像の明るさが近くのフレーム内で一定のままであるということです。残念ながら、内視鏡シーンは、照明の変動、非ランバート反射、およびデータ収集中の相互反射によって引き起こされる深刻な明るさの変動があり、これらの明るさの変動は必然的に深度と自我運動の推定精度を低下させるため、この仮定を満たしていません。この作品では、明るさの不一致の問題に対処するために、外観フローと呼ばれる新しい概念を紹介します。外観の流れは、明るさのパターンの変化を考慮に入れて、一般化された動的画像制約を開発することを可能にします。さらに、構造モジュール、運動モジュール、外観モジュール、対応モジュールで構成される内視鏡シーンで単眼深度と自我運動を同時に推定する統合自己監視フレームワークを構築し、外観を正確に再構築して画像の明るさを調整します。 SCAREDデータセットとEndoSLAMデータセットで広範な実験が行われ、提案された統合フレームワークは、他の自己監視型アプローチを大幅に上回っています。さまざまな患者やカメラでフレームワークの一般化能力を検証するために、SCAREDでモデルをトレーニングしますが、微調整なしでSERV-CTおよびHamlynデータセットでテストします。優れた結果により、強力な一般化能力が明らかになります。コードは次のURLで入手できます：\ url {https://github.com/ShuweiShao/AF-SfMLearner}。
--------------------------------------------------


## Vision Transformer Based Video Hashing Retrieval for Tracing the Source of Fake Videos
偽のビデオのソースを追跡するためのVisionTransformerベースのビデオハッシュ検索 

- http://arxiv.org/pdf/2112.08117v1
### summary
従来の偽のビデオ検出方法は、改ざんされた画像の可能性の値または疑わしいマスクを出力します。しかし、そのような説明のつかない結果は、説得力のある証拠として使用することはできません。したがって、偽のビデオのソースを追跡することをお勧めします。従来のハッシュ手法は、画像のニュアンスを区別できないセマンティック類似画像を取得するために使用されます。具体的には、従来のビデオ検索と比較したソーストレース。同様のソースビデオから本物を見つけるのは難しいことです。私たちは、人々のビデオが非常に似ているという問題を解決するために、新しい損失HashTriplet Lossを設計しました。異なる角度の同じシーン、同じ人物の類似したシーンです。 Video Tracing and TamperingLocalization（VTL）という名前のVisionTransformerベースのモデルを提案します。最初の段階では、ViTHash（VTL-T）によってハッシュセンターをトレーニングします。次に、偽のビデオがViTHashに入力され、ViTHashがハッシュコードを出力します。ハッシュコードは、ハッシュセンターからソースビデオを取得するために使用されます。第2段階では、ソースビデオとフェイクビデオがジェネレータ（VTL-L）に入力されます。次に、疑わしい領域をマスクして補助情報を提供します。さらに、DFTLとDAVIS2016-TLの2つのデータセットを作成しました。 DFTLでの実験は、類似したビデオのソーストレースにおけるフレームワークの優位性を明確に示しています。特に、VTLは、DAVIS2016-TLの最先端の方法と同等のパフォーマンスも達成しました。ソースコードとデータセットはGitHubでリリースされています：\ url {https://github.com/lajlksdf/vtl}。
--------------------------------------------------


## Image-Adaptive YOLO for Object Detection in Adverse Weather Conditions
悪天候下での物体検出のための画像適応型YOLO 

- http://arxiv.org/pdf/2112.08088v1
### summary
ディープラーニングベースのオブジェクト検出方法は、従来のデータセットで有望な結果を達成しましたが、悪天候でキャプチャされた低品質の画像からオブジェクトを見つけることは依然として困難です。既存の方法は、画像強調とオブジェクト検出のタスクのバランスを取るのが難しいか、検出に有益な潜在的な情報を無視することがよくあります。この問題を軽減するために、新しい画像適応型YOLO（IA-YOLO）フレームワークを提案します。このフレームワークでは、各画像を適応的に拡張して、検出パフォーマンスを向上させることができます。具体的には、微分可能画像処理（DIP）モジュールが提示され、YOLO検出器の悪天候条件を考慮に入れます。この検出器のパラメーターは、小さな畳み込みニューラルネットワーク（CNN-PP）によって予測されます。 CNN-PPとYOLOv3をエンドツーエンドで共同で学習します。これにより、CNN-PPは適切なDIPを学習して、弱く監視された方法で検出用の画像を強化できます。私たちが提案するIA-YOLOアプローチは、通常の気象条件と悪天候の条件の両方で画像を適応的に処理できます。実験結果は非常に有望であり、霧のシナリオと暗いシナリオの両方で提案されたIA-YOLOメソッドの有効性を示しています。
--------------------------------------------------


## Depth Refinement for Improved Stereo Reconstruction
ステレオ再構成を改善するための深さの調整 

- http://arxiv.org/pdf/2112.08070v1
### summary
深度推定は、ロボット工学、拡張現実、自律運転など、環境の3D評価を必要とする膨大な数のアプリケーションの基礎です。深度推定の優れた手法の1つは、ステレオマッチングです。これには、他の深度検知技術よりもアクセスしやすく、リアルタイムで高密度の深度推定を生成でき、近年の深層学習の進歩から大きな恩恵を受けているといういくつかの利点があります。ただし、立体画像から深度を推定するための現在の手法には、依然として組み込みの欠点があります。深度を再構築するために、ステレオマッチングアルゴリズムは、幾何学的三角測量を適用する前に、最初に左右の画像間の視差マップを推定します。簡単な分析により、深度エラーはオブジェクトの距離に二次的に比例することがわかります。したがって、一定の視差エラーは、カメラから遠く離れたオブジェクトの大きな深度エラーに変換されます。この二次関係を緩和するために、深さ推定にリファインメントネットワークを使用するシンプルで効果的な方法を提案します。提案された学習手順がこの二次関係を減らすことを示唆する分析的および経験的結果を示します。提案された改良手順を、SceneflowやKITTIデータセットなどのよく知られたベンチマークやデータセットで評価し、深度精度メトリックの大幅な改善を示します。
--------------------------------------------------


## Leveraging Image-based Generative Adversarial Networks for Time Series Generation
時系列生成のための画像ベースの生成的敵対的ネットワークの活用 

- http://arxiv.org/pdf/2112.08060v1
### summary
生成モデルは、サンプリング品質、多様性、および機能の解きほぐしに関して大きな成功を収めて画像データを合成します。時系列の生成モデルは、時間的ダイナミクスをキャプチャし、サンプリングの反転を可能にする表現が欠落しているため、これらの利点がありません。この論文は、時系列生成のための画像ベースの生成的敵対的ネットワークの使用を容易にするために、時間間リターンプロット（IRP）表現を提案します。表現は、時系列の特性をキャプチャするのに効果的であり、代替表現と比較して、可逆性とスケール不変性の恩恵を受けます。経験的ベンチマークはこれらの機能を確認し、IRPが勾配ペナルティを備えた既製のWasserstein GANを有効にして、特殊なRNNベースのGANを上回ると同時に、モデルの複雑さを軽減する現実的な時系列をサンプリングできることを示しています。
--------------------------------------------------


## Exploring the Asynchronous of the Frequency Spectra of GAN-generated Facial Images
GANで生成された顔画像の周波数スペクトルの非同期性の調査 

- http://arxiv.org/pdf/2112.08050v1
### summary
Generative Adversarial Networks（GAN）の急速な進歩により、悪意のある目的、特に偽の顔画像の作成における悪用の懸念が高まっています。提案された多くの方法はGANベースの合成画像の検出に成功しますが、大量のトレーニング偽画像データセットの必要性と、検出器の未知の顔画像への一般化の課題によって制限されています。この論文では、カラーチャネルの非同期周波数スペクトルを調査する新しいアプローチを提案します。これは、GANベースの合成画像を区別するために、教師なし学習モデルと教師なし学習モデルの両方をトレーニングするのに簡単ですが効果的です。さらに、あるソースドメインで提案された機能から学習し、機能の分布について事前に知っている別のターゲットドメインで検証する、トレーニングモデルの転送可能性を調査します。私たちの実験結果は、周波数領域でのスペクトルの不一致が、さまざまなタイプのGANベースの生成画像を効果的に検出するための実用的なアーティファクトであることを示しています。
--------------------------------------------------


## LookinGood^π: Real-time Person-independent Neural Re-rendering for High-quality Human Performance Capture
LookinGood ^π：高品質の人間のパフォーマンスキャプチャのためのリアルタイムの人に依存しないニューラルリレンダリング 

- http://arxiv.org/pdf/2112.08037v1
### summary
LookinGood ^ {\ pi}を提案します。これは、（1）人間のパフォーマンスキャプチャシステムからの低品質の再構成結果のレンダリング品質をリアルタイムで改善することを目的とした新しいニューラル再レンダリングアプローチです。 （2）見えない人のニューラルレンダリングネットワークの一般化能力を向上させる。私たちの重要なアイデアは、再構成されたジオメトリのレンダリングされた画像をガイダンスとして利用して、いくつかの参照画像からの個人固有の詳細の予測を支援し、再レンダリングされた結果を強化することです。これに照らして、2分岐ネットワークを設計します。粗いブランチは、いくつかのアーティファクト（つまり、穴、ノイズ）を修正し、レンダリングされた入力の粗いバージョンを取得するように設計されています。一方、詳細ブランチは、ワープされた参照から「正しい」詳細を予測するように設計されています。レンダリングされた画像のガイダンスは、ディテールブランチのトレーニングで2つのブランチの特徴を効果的にブレンドすることで実現され、ワー​​ピングの精度とディテールの忠実度の両方が向上します。私たちの方法は、目に見えない人々に忠実な画像を生成する際に、最先端の方法よりも優れていることを示しています。
--------------------------------------------------


## Segmentation-Reconstruction-Guided Facial Image De-occlusion
セグメンテーション-再構成-ガイド付き顔画像のオクルージョン解除 

- http://arxiv.org/pdf/2112.08022v1
### summary
閉塞は、野生の顔画像で非常に一般的であり、顔関連のタスクのパフォーマンスを低下させます。顔画像からオクルージョンを削除するために多くの努力が注がれていますが、オクルージョンのさまざまな形状とテクスチャは、現在の方法の堅牢性に依然として挑戦しています。その結果、現在の方法は、手動のオクルージョンマスクに依存するか、特定のオクルージョンにのみ適用されます。この論文は、顔のセグメンテーションと3D顔再構成に基づく新しい顔のオクルージョン解除モデルを提案します。これは、髪の毛など、境界がぼやけているすべての種類の顔のオクルージョンを自動的に削除します。提案されたモデルは、3D顔再構成モジュール、顔セグメンテーションモジュール、および画像生成モジュールで構成されています。最初の2つによってそれぞれ前の顔とオクルージョンマスクが予測されると、画像生成モジュールは欠落している顔のテクスチャを忠実に復元できます。トレーニングを監督するために、手動でラベル付けされたオクルージョンと合成オクルージョンの両方を使用して、大規模なオクルージョンデータセットをさらに構築します。定性的および定量的結果は、提案された方法の有効性と堅牢性を示しています。
--------------------------------------------------


## MissMarple : A Novel Socio-inspired Feature-transfer Learning Deep Network for Image Splicing Detection
MissMarple：画像スプライシング検出のための新しい社会に触発された機能転送学習ディープネットワーク 

- http://arxiv.org/pdf/2112.08018v1
### summary
この論文では、画像スプライシング検出のための新しい社会に触発された畳み込みニューラルネットワーク（CNN）深層学習モデルを提案します。粗くスプライスされた画像領域の検出から学習することで、視覚的に知覚できない細かくスプライスされた画像偽造の検出を改善できるという前提に基づいて、MissMarpleと呼ばれる提案されたモデルは、特徴伝達学習を含むツインCNNネットワークです。コロンビアスプライシング、WildWeb、DSO1などのベンチマークデータセットと現実的なスプライシング偽造からなるAbhASというタイトルの提案データセットを使用して提案モデルをトレーニングおよびテストして得られた結果は、既存の深層学習モデルよりも検出精度が向上していることを明らかにしました。
--------------------------------------------------


## Consistent Depth Prediction under Various Illuminations using Dilated Cross Attention
拡張クロスアテンションを使用したさまざまな照明下での一貫した深さ予測 

- http://arxiv.org/pdf/2112.08006v1
### summary
本論文では、様々な照明条件下での一貫した深度予測の複雑なシーンの問題を解決することを目的としている。 RGB-Dセンサーまたは仮想レンダリングに基づく既存の屋内データセットには、2つの重要な制限があります。スパース深度マップ（NYU Depth V2）と非現実的な照明（SUN CG、SceneNet RGB-D）です。インターネット3D屋内シーンを使用し、それらの照明を手動で調整して、フォトリアリスティックなRGB写真とそれに対応する深度およびBRDFマップをレンダリングし、Variデータセットと呼ばれる新しい屋内深度データセットを取得することを提案します。グローバル情報を処理し、パラメータを削減するために、エンコードされた特徴に深さ方向に分離可能な拡張畳み込みを適用することにより、DCAという名前の単純な畳み込みブロックを提案します。さまざまな照明の下で深度予測の一貫性を維持するために、これらの拡張された機能に対してクロスアテンションを実行します。私たちの方法は、Variデータセットの現在の最先端の方法と比較することによって評価され、実験で大幅な改善が見られます。また、アブレーションスタディを実施し、NYU Depth V2でモデルを微調整し、実際のデータで評価して、DCAブロックの有効性をさらに検証します。コード、事前トレーニング済みの重み、およびVaridatasetはオープンソースです。
--------------------------------------------------


## Autoencoder-based background reconstruction and foreground segmentation with background noise estimation
オートエンコーダベースのバックグラウンド再構成とバックグラウンドノイズ推定によるフォアグラウンドセグメンテーション 

- http://arxiv.org/pdf/2112.08001v1
### summary
何十年にもわたる研究の後でも、動的なシーンの背景の再構築と前景のオブジェクトのセグメンテーションは、照明の変化、カメラの動き、乱気流や樹木の移動によって引き起こされる背景ノイズなど、さまざまな課題があるため、未解決の問題と見なされています。この論文では、オートエンコーダを使用してビデオシーケンスの背景を低次元マニホールドとしてモデル化し、このオートエンコーダによって提供される再構成された背景を元の画像と比較して、前景/背景セグメンテーションマスクを計算することを提案します。提案されたモデルの主な目新しさは、自動エンコーダがバックグラウンドノイズを予測するようにトレーニングされていることです。これにより、フレームごとにピクセル依存のしきい値を計算して、バックグラウンド/フォアグラウンドセグメンテーションを実行できます。提案されたモデルは時間情報や動きの情報を使用しませんが、CDnet 2014およびLASIESTAデータセットの監視されていないバックグラウンド減算の最先端を超えており、カメラが動いているビデオが大幅に改善されています。
--------------------------------------------------


## Self-Ensembling GAN for Cross-Domain Semantic Segmentation
クロスドメインセマンティックセグメンテーションのための自己アンサンブルGAN 

- http://arxiv.org/pdf/2112.07999v1
### summary
ディープニューラルネットワーク（DNN）は、セマンティックセグメンテーションのパフォーマンス向上に大きく貢献しています。それにもかかわらず、DNNのトレーニングには通常、大量のピクセルレベルのラベル付きデータが必要であり、実際に収集するには費用と時間がかかります。注釈の負担を軽減するために、この論文では、セマンティックセグメンテーションにクロスドメインデータを利用する自己アンサンブル生成的敵対的ネットワーク（SE-GAN）を提案します。 SE-GANでは、教師ネットワークと学生ネットワークが、セマンティックセグメンテーションマップを生成するための自己アンサンブルモデルを構成します。これは、弁別子とともにGANを形成します。その単純さにもかかわらず、SE-GANは敵対的トレーニングのパフォーマンスを大幅に向上させ、モデルの安定性を高めることができます。後者は、ほとんどの敵対的トレーニングベースの方法で共有される一般的な障壁です。 SE-GANを理論的に分析し、$ \ mathcal O（1 / \ sqrt {N}）$一般化限界（$ N $はトレーニングサンプルサイズ）を提供します。これは、一般化可能性を強化するために弁別子の仮説の複雑さを制御することを示唆しています。したがって、弁別器として単純なネットワークを選択します。 2つの標準設定での広範囲にわたる体系的な実験は、提案された方法が現在の最先端のアプローチを大幅に上回っていることを示しています。モデルのソースコードはまもなく利用可能になります。
--------------------------------------------------


## Temporal Action Proposal Generation with Background Constraint
バックグラウンド制約のある時間的アクション提案の生成 

- http://arxiv.org/pdf/2112.07984v1
### summary
時間的アクション提案の生成（TAPG）は、時間的境界のあるトリミングされていないビデオ内のアクションインスタンスを見つけることを目的とした挑戦的なタスクです。提案の信頼性を評価するために、既存の作業は通常、提案とグラウンドトゥルースの間のtemporalIntersection-over-Union（tIoU）によって監視される提案のアクションスコアを予測します。この論文では、提案の信頼性を制限するために背景予測スコアを利用することにより、低品質の提案をさらに抑制するための一般的な補助的な背景制約のアイデアを革新的に提案します。このようにして、背景制約の概念を既存のTAPGメソッド（BMN、GTADなど）に簡単にプラグインして再生できます。この観点から、アクションとバックグラウンドの豊富な情報をさらに活用するために、BackgroundConstraint Network（BCNet）を提案します。具体的には、信頼性の高い信頼性評価のためのAction-BackgroundInteractionモジュールを導入します。このモジュールは、フレームおよびクリップレベルでの注意メカニズムによって、アクションとバックグラウンドの間の不整合をモデル化します。 2つの人気のあるベンチマーク、つまりActivityNet-1.3とTHUMOS14で広範な実験が行われます。結果は、私たちの方法が最先端の方法よりも優れていることを示しています。既存のアクション分類器を備えたこのメソッドは、一時的なアクションのローカリゼーションタスクでも優れたパフォーマンスを実現します。
--------------------------------------------------


## Detail-aware Deep Clothing Animations Infused with Multi-source Attributes
マルチソース属性が注入された詳細を意識した深みのある服のアニメーション 

- http://arxiv.org/pdf/2112.07974v1
### summary
この論文は、さまざまなアニメーションでさまざまな形状の体が着用する衣服の豊かで合理的な詳細な変形を生成するための、新しい学習ベースの衣服変形方法を提示します。さまざまな衣服のトポロジーやポーズに対して多数のトレーニング済みモデルを必要とし、豊富な詳細を簡単に実現できない既存の学習ベースの方法とは対照的に、統一されたフレームワークを使用して、高忠実度の変形を効率的かつ簡単に生成します。マルチソース属性の影響を受ける変形を予測するという難しい問題に対処するために、新しい視点から3つの戦略を提案します。具体的には、衣服と体のフィット感が折り目の程度に重要な影響を与えることを最初に発見しました。次に、属性パーサーを設計して詳細認識エンコーディングを生成し、それらをグラフニューラルネットワークに注入して、多様な属性の下での詳細の識別を強化しました。さらに、より良い収束を達成し、過度に滑らかな変形を回避するために、学習タスクの複雑さを軽減するために出力の再構築を提案しました。実験結果は、提案された変形方法が一般化可能性と詳細の品質の点で既存の方法よりも優れた性能を達成することを示しています。
--------------------------------------------------


## Predicting Media Memorability: Comparing Visual, Textual and Auditory Features
メディアの記憶力の予測：視覚的、テキスト的、聴覚的特徴の比較 

- http://arxiv.org/pdf/2112.07969v1
### summary
このホワイトペーパーでは、MediaEval 2021のメディア記憶力の予測タスクへのアプローチについて説明します。これは、ビデオの記憶力を自動的に予測するタスクを設定することにより、メディアの記憶力の問題に対処することを目的としています。今年は、比較の観点から課題に取り組み、3つの調査されたモダリティのそれぞれについてより深い洞察を得ることを目指し、昨年の提出（2020）の結果を参照点として使用します。 TRECVid2019データセットでテストされた最高のパフォーマンスの短期記憶力モデル（0.132）は、昨年と同様に、TRECVidデータでトレーニングされていないフレームベースのCNNであり、Memento10kでテストされた最高の短期記憶力モデル（0.524）でした。データセットは、DenseNet121の視覚的機能に適合したベイジアンライドリグレッサでした。
--------------------------------------------------


## Modality-Aware Triplet Hard Mining for Zero-shot Sketch-Based Image Retrieval
ゼロショットスケッチベースの画像検索のためのモダリティ対応トリプレットハードマイニング 

- http://arxiv.org/pdf/2112.07966v1
### summary
この論文は、クロスモダリティメトリック学習の観点からゼロショットスケッチベースの画像検索（ZS-SBIR）問題に取り組んでいます。 ％ディープメトリック学習の最近のグッドプラクティス。このタスクには2つの特徴があります。1）ゼロショット設定では、新しいクラスを認識するために、クラス内のコンパクトさとクラス間の不一致が良好な距離空間が必要です。2）スケッチクエリとフォトギャラリーは異なるモダリティにあります。メトリック学習の観点は、2つの側面からZS-SBIRにメリットをもたらします。まず、ディープメトリックラーニング（DML）の最近のグッドプラクティスを通じて改善を促進します。 DMLの2つの基本的な学習アプローチ、\ emph {e.g。}、分類トレーニングとペアワイズトレーニングを組み合わせることにより、ZS-SBIRの強力なベースラインを設定しました。ベルやホイッスルがない場合、このベースラインは競争力のある検索精度を実現します。第二に、モダリティギャップを適切に抑制することが重要であるという洞察を提供します。この目的のために、Modality-Aware Triplet Hard Mining（MATHM）という名前の新しいメソッドを設計します。 MATHMは、3種類のペアワイズ学習、\ emph {eg}、クロスモダリティサンプルペア、アウィズモダリティサンプルペア、およびそれらの組み合わせでベースラインを強化します。\また、トレーニング中にこれら3つのコンポーネントのバランスを動的にとる適応型重み付け方法を設計します。実験的結果は、MATHMが強力なベースラインに基づいて大幅な改善をもたらし、新しい最先端のパフォーマンスを確立することを確認しています。たとえば、TU-Berlinデータセットでは、47.88 + 2.94 \％mAP @allと58.28+ 2.34 \％Prec @ 100を達成します。コードは公開されます。
--------------------------------------------------


## Towards General and Efficient Active Learning
一般的かつ効率的なアクティブラーニングに向けて 

- http://arxiv.org/pdf/2112.07963v1
### summary
アクティブラーニングは、限られた注釈予算を活用するために最も有益なサンプルを選択することを目的としています。ほとんどの既存の作業は、時間のかかるモデルトレーニングとバッチデータの選択を各データセットで個別に複数回繰り返すことにより、面倒なパイプラインに従います。この論文では、新しい一般的で効率的なアクティブラーニング（GEAL）手法を提案することで、この現状に挑戦します。大規模なデータセットで事前にトレーニングされた公開モデルを利用して、この手法では、さまざまなデータセットでデータ選択プロセスを実行できます。同じモデル。画像内の微妙なローカル情報をキャプチャするために、事前にトレーニングされたネットワークの中間機能から簡単に抽出できる知識クラスターを提案します。面倒なバッチ選択戦略の代わりに、きめ細かい知識クラスターレベルでK-Center-Greedyを実行することにより、すべてのデータサンプルが一度に選択されます。手順全体は、トレーニングや監督なしでシングルパスモデルの推論のみを必要とし、時間の複雑さの点で、私たちの方法を従来技術よりも最大数百倍も著しく優れたものにします。広範な実験は、オブジェクト検出、セマンティックセグメンテーション、深度推定、および画像分類に関する私たちの方法の有望なパフォーマンスを広く示しています。
--------------------------------------------------


## A learning-based approach to feature recognition of Engineering shapes
工学的形状の特徴認識への学習ベースのアプローチ 

- http://arxiv.org/pdf/2112.07962v1
### summary
この論文では、CADメッシュモデルの穴やスロットなどのエンジニアリング形状の特徴を認識するための機械学習アプローチを提案します。デジタルアーカイブ、3D印刷、コンポーネントのスキャン、リバースエンジニアリングなどの新しい製造技術の出現により、CADデータはメッシュモデル表現の形で急増しています。メッシュモデルではノードとエッジの数が多くなり、ノイズが存在する可能性があるため、グラフベースのアプローチを直接適用すると、コストがかかるだけでなく、ノイズの多いデータに合わせて調整することも困難になります。したがって、これには、メッシュの形式で表されるCADモデルの特徴認識のために考案される新しいアプローチが必要です。ここでは、ガウス写像の離散バージョンを特徴学習のシグネチャとして使用できることを示します。このアプローチでは、必要なメモリ要件が少ないだけでなく、トレーニング時間もかなり短いことを示しています。ネットワークアーキテクチャが関与していないため、ハイパーパラメータの数ははるかに少なく、はるかに高速に調整できます。認識の精度も、3D畳み込みニューラルネットワーク（CNN）を使用して得られたものと非常に似ていますが、実行時間とストレージ要件がはるかに少なくなります。他の非ネットワークベースの機械学習アプローチとの比較が行われ、私たちのアプローチが最高の精度を持っていることが示されました。また、公開ベンチマークから得られた複雑な/相互作用する機能だけでなく、複数の機能を備えたCADモデルの認識結果も示します。ノイズの多いデータを処理する機能も実証されています。
--------------------------------------------------


## FEAR: Fast, Efficient, Accurate and Robust Visual Tracker
恐れ：高速、効率的、正確で堅牢なビジュアルトラッカー 

- http://arxiv.org/pdf/2112.07957v1
### summary
FEARは、斬新で、高速で、効率的で、正確で、堅牢なSiamesevisualトラッカーです。デュアルテンプレート表現と呼ばれるオブジェクトモデル適応用のアーキテクチャブロックと、モデルの柔軟性と効率を高めるためのピクセル単位の融合ブロックを紹介します。デュアルテンプレートモジュールは、単一の学習可能なパラメーターのみで時間情報を組み込みますが、ピクセル単位の融合ブロックは、標準の相関モジュールと比較して、より少ないパラメーターでより識別力のある特徴をエンコードします。洗練されていないバックボーンを新しいモジュールでプラグインすることにより、FEAR-MおよびFEAR-Lトラッカーは、精度と効率の両方で、いくつかの学術的ベンチマークでほとんどのSiamesetrackerを上回ります。軽量のバックボーンを採用した最適化されたversionFEAR-XSは、最新の結果に近い状態を維持しながら、現在のシャムトラッカーよりも10倍以上高速なトラッキングを提供します。 FEAR-XSトラッカーは、LightTrack [62]より2.4倍小さく、4.3倍高速で、優れた精度を備えています。さらに、エネルギー消費と実行速度のベンチマークを導入することにより、モデル効率の定義を拡張します。ソースコード、事前にトレーニングされたモデル、および評価プロトコルは、リクエストに応じて利用可能になります
--------------------------------------------------


## Object Pursuit: Building a Space of Objects via Discriminative Weight Generation
オブジェクトの追跡：識別可能な重みの生成によるオブジェクトのスペースの構築 

- http://arxiv.org/pdf/2112.07954v1
### summary
視覚的な学習と理解のために、オブジェクト中心の表現を継続的に学習するためのフレームワークを提案します。既存のオブジェクト中心の表現は、シーン内のオブジェクトを個別化する監視に依存するか、現実世界の複雑なシーンをほとんど処理できない監視されていない解きほぐしを実行します。注釈の負担を軽減し、データの統計的複雑さに対する制約を緩和するために、私たちの方法は、相互作用を活用して、オブジェクト中心の表現を学習しながら、オブジェクトの多様なバリエーションと対応するトレーニング信号を効果的にサンプリングします。学習を通じて、オブジェクトは未知のIDでランダムな順序で1つずつストリーミングされ、畳み込みハイパーネットワークを介して各オブジェクトの識別重みを合成できる潜在コードに関連付けられます。さらに、学習されたオブジェクトの再識別と忘却防止が、学習プロセスを効率的にするために採用されます。堅牢です。提案されたフレームワークの主要な機能の次の研究を実行し、学習した表現の特性を分析します。さらに、下流のタスクでラベルの効率を向上させることができる表現を学習する際に提案されたフレームワークの機能を示します。私たちのコードと訓練されたモデルは公開されます。
--------------------------------------------------


## Transcoded Video Restoration by Temporal Spatial Auxiliary Network
時間的空間補助ネットワークによるトランスコードされたビデオの復元 

- http://arxiv.org/pdf/2112.07948v1
### summary
YoutubeやTikTokなどのほとんどのビデオプラットフォームでは、再生されるビデオは通常、録画デバイスによるハードウェアエンコーディング、ビデオ編集アプリによるソフトウェアエンコーディング、ビデオアプリケーションサーバーによる単一/複数のビデオトランスコーディングなど、複数のビデオエンコーディングを受けています。圧縮ビデオ復元の以前の作業では、通常、圧縮アーティファクトが1回限りのエンコーディングによって引き起こされると想定しています。したがって、導出されたソリューションは通常、実際にはあまりうまく機能しません。本論文では、トランスコードされたビデオ復元のための新しい方法、時間空間補助ネットワーク（TSAN）を提案する。私たちの方法は、ビデオエンコーディングとトランスコーディングの間の固有の特性を考慮し、ネットワークが自己監視注意トレーニングを実施するのを支援するための中間ラ​​ベルとして、最初の浅いエンコードされたビデオを考慮します。さらに、隣接するマルチフレーム情報を採用し、トランスコードされたビデオ復元のための時間的変形可能アライメントとピラミッド型空間融合を提案します。実験結果は、提案された方法の性能が以前の技術の性能よりも優れていることを示している。コードはhttps://github.com/icecherylXuli/TSANで入手できます。
--------------------------------------------------


## Efficient Geometry-aware 3D Generative Adversarial Networks
効率的なジオメトリ対応の3D生成的敵対的ネットワーク 

- http://arxiv.org/pdf/2112.07945v1
### summary
シングルビューの2D写真のコレクションのみを使用して、高品質のマルチビュー整合性のある画像と3Dシェイプを教師なしで生成することは、長年の課題でした。既存の3DGANは、計算集約型であるか、3D整合性のない近似を作成します。前者は生成された画像の品質と解像度を制限し、後者はマルチビューの一貫性と形状の品質に悪影響を及ぼします。この作業では、これらの近似に過度に依存することなく、3DGANの計算効率と画質を向上させます。この目的のために、表現力豊かなハイブリッド明示的暗黙的ネットワークアーキテクチャを紹介します。これは、他の設計上の選択肢とともに、高解像度のマルチビュー整合性のある画像をリアルタイムで合成するだけでなく、高品質の3Dジオメトリを生成します。機能生成とニューラルレンダリングを分離することにより、私たちのフレームワークは、StyleGAN2などの最先端の2D CNNジェネレーターを活用し、それらの効率と表現力を継承することができます。他の実験の中でも、FFHQおよびAFHQCatsを使用した最先端の3D認識合成を示します。
--------------------------------------------------


## From Noise to Feature: Exploiting Intensity Distribution as a Novel Soft Biometric Trait for Finger Vein Recognition
ノイズから特徴へ：指静脈認識のための新しいソフト生体認証特性としての強度分布の活用 

- http://arxiv.org/pdf/2112.07931v1
### summary
ほとんどの指静脈特徴抽出アルゴリズムは、指組織によって形成される強度分布を同時に無視し、場合によってはそれをバックグラウンドノイズとして処理するにもかかわらず、テクスチャ表現能力により満足のいくパフォーマンスを実現します。この論文では、より良い指静脈認識性能を達成するための新しいソフト生体認証特性として、この種のノイズを利用します。最初に、指静脈イメージングの原理と画像の特性の詳細な分析を提示して、背景の指組織によって形成される強度分布を、認識のためのソフトな生体認証特性として抽出できることを示します。次に、強度分布特徴抽出のために、２つの指静脈背景層抽出アルゴリズムおよび３つのソフト生体特性抽出アルゴリズムが提案される。最後に、ハイブリッドマッチング戦略を提案して、スコアレベルでのプライマリバイオメトリック特性とソフトバイオメトリック特性の間の寸法差の問題を解決します。 3つのオープンアクセスデータベースでの一連の厳密なコントラスト実験は、提案された方法が実行可能で効果的な指静脈認識であることを示しています。
--------------------------------------------------


## Imagine by Reasoning: A Reasoning-Based Implicit Semantic Data Augmentation for Long-Tailed Classification
推論による想像：長い尾の分類のための推論ベースの暗黙の意味データ拡張 

- http://arxiv.org/pdf/2112.07928v1
### summary
実際のデータはロングテール分布に従うことが多く、既存の分類アルゴリズムのパフォーマンスが大幅に低下します。重要な問題は、尾のカテゴリのサンプルがクラス内の多様性を表現できないことです。人間は、このカテゴリを初めて表示する場合でも、事前の知識を使用して、新しいポーズ、シーン、および視野角のサンプルを想像できます。これに触発されて、他のクラスから変換方向を借用するための新しい推論ベースの暗黙の意味データ拡張メソッドを提案します。各カテゴリの共分散行列は特徴の変換方向を表すため、類似したカテゴリから生成された完全に異なるインスタンスへの新しい方向をサンプリングできます。具体的には、バックボーンと分類器をトレーニングするために、ロングテール分散データが最初に採用されます。次に、各カテゴリの共分散行列が推定され、任意の2つのカテゴリの関係を格納するための知識グラフが作成されます。最後に、テールサンプルは、知識グラフ内のすべての同様のカテゴリからの情報を伝播することによって適応的に強化されます。 CIFAR-100-LT、ImageNet-LT、およびiNaturalist 2018の実験結果は、最先端の方法と比較して、提案された方法の有効性を示しています。
--------------------------------------------------


## Temporal Shuffling for Defending Deep Action Recognition Models against Adversarial Attacks
敵対的攻撃からディープアクション認識モデルを防御するための時間的シャッフル 

- http://arxiv.org/pdf/2112.07921v1
### summary
最近、畳み込みニューラルネットワーク（CNN）を使用したビデオベースのアクション認識方法により、優れた認識パフォーマンスが実現されています。ただし、アクション認識モデルの一般化メカニズムについてはまだ理解が不足しています。この論文では、アクション認識モデルが予想よりも少ないモーション情報に依存しているため、フレーム次数のランダム化に対してロバストであることを提案します。この観察に基づいて、アクション認識モデルの敵対者に対する入力ビデオの一時的なシャッフルを使用した新しい防御方法を開発します。私たちの防御方法を可能にする別の観察は、ビデオの敵対的な摂動が一時的な破壊に敏感であるということです。私たちの知る限り、これはビデオベースのアクション認識モデルに固有の防御方法を設計する最初の試みです。
--------------------------------------------------


## M-FasterSeg: An Efficient Semantic Segmentation Network Based on Neural Architecture Search
M-FasterSeg：ニューラルアーキテクチャ検索に基づく効率的なセマンティックセグメンテーションネットワーク 

- http://arxiv.org/pdf/2112.07918v1
### summary
画像セマンティックセグメンテーション技術は、インテリジェントシステムが自然のシーンを理解するための重要な技術の1つです。ビジュアルインテリジェンスの分野における重要な研究の方向性の1つとして、このテクノロジーは、移動ロボット、ドローン、スマートドライブ、およびスマートセキュリティの分野で幅広いアプリケーションシナリオを持っています。ただし、モバイルロボットの実際のアプリケーションでは、不正確なセグメンテーションセマンティックラベル予測や、セグメント化されたオブジェクトと背景のエッジ情報の喪失などの問題が発生する可能性があります。本論文は、自己注意ニューラルネットワークとニューラルネットワークアーキテクチャ検索方法を組み合わせた深層学習ネットワークに基づくセマンティックセグメンテーションネットワークの改良された構造を提案する。まず、ニューラルネットワーク検索方法NAS（Neural Architecture Search）を使用して、複数の解像度ブランチを持つセマンティックセグメンテーションネットワークを検索します。検索プロセスでは、自己注意ネットワーク構造モジュールを組み合わせて検索されたニューラルネットワーク構造を調整し、次に異なるブランチによって検索されたセマンティックセグメンテーションネットワークを組み合わせて高速セマンティックセグメンテーションネットワーク構造を形成し、画像をネットワーク構造に入力して最終的な予測を取得します結果：Cityscapesデータセットの実験結果は、アルゴリズムの精度が69.8％であり、セグメンテーション速度が48 / sであることを示しています。リアルタイムと精度のバランスが取れており、エッジのセグメンテーションを最適化でき、複雑なシーンでのパフォーマンスが向上します。優れた堅牢性は、実用的なアプリケーションに適しています。
--------------------------------------------------


## SPTS: Single-Point Text Spotting
SPTS：シングルポイントテキストスポッティング 

- http://arxiv.org/pdf/2112.07917v1
### summary
ほとんどすべてのシーンテキストスポッティング（検出および認識）方法は、コストのかかるボックス注釈（テキスト行ボックス、単語レベルボックス、文字レベルボックスなど）に依存しています。初めて、トレーニングシーンのテキストスポッティングモデルが、インスタンスごとに単一ポイントの非常に低コストの注釈を使用して実現できることを示します。言語モデリングのように、シーケンス予測タスクとしてシーンテキストスポッティングに取り組むエンドツーエンドのシーンテキストスポッティング方法を提案します。入力として画像が与えられた場合、目的の検出と認識の結果を離散トークンのシーケンスとして定式化し、自動回帰トランスフォーマーを使用してシーケンスを予測します。いくつかの水平、多方向、および任意の形状のシーンテキストベンチマークで有望な結果を達成します。最も重要なことは、パフォーマンスがポイント注釈の位置にあまり敏感ではないことを示しています。つまり、正確な位置を必要とするバウンディングボックスよりも、注釈を付けて自動的に生成する方がはるかに簡単です。このような先駆的な試みは、これまで可能であったよりもはるかに大規模なシーンテキストスポッティングアプリケーションの重要な機会を示していると私たちは信じています。
--------------------------------------------------


## A Comparative Analysis of Machine Learning Approaches for Automated Face Mask Detection During COVID-19
COVID-19中の自動フェイスマスク検出のための機械学習アプローチの比較分析 

- http://arxiv.org/pdf/2112.07913v1
### summary
世界保健機関（WHO）は、COVID-19感染を防ぐための最も効果的な対策の1つとして、フェイスマスクの着用を推奨しています。多くの国では、特に公共の場所でフェイスマスクを着用することが義務付けられています。フェイスマスクの手動監視は群衆の真ん中で実行できないことが多いため、自動検出が有益な場合があります。これを容易にするために、顔マスク検出用の多数の深層学習モデル（VGG1、VGG19、ResNet50）を調査し、2つのベンチマークデータセットで評価しました。また、このコンテキストで転送学習（つまり、VGG19、ImageNetで事前トレーニングされたResNet50）を評価しました。すべてのモデルのパフォーマンスは非常に優れていますが、転移学習モデルが最高のパフォーマンスを達成していることがわかります。転送学習により、トレーニング時間が30 \％短縮され、パフォーマンスが0.10 \％-0.40 \％向上します。また、私たちの実験では、これらの高性能モデルは、テストデータセットが異なる分布からのものである実際のケースでは、それほど堅牢ではないことも示しています。微調整を行わないと、これらのモデルのパフォーマンスは、ドメイン間設定で47 \％低下します。
--------------------------------------------------


## Decoupling Zero-Shot Semantic Segmentation
ゼロショットセマンティックセグメンテーションのデカップリング 

- http://arxiv.org/pdf/2112.07910v1
### summary
ゼロショットセマンティックセグメンテーション（ZS3）は、トレーニングで見られなかった新しいカテゴリをセグメント化することを目的としています。既存の作品は、ZS3をピクセルレベルのゼロショット分類問題として定式化し、テキストのみで事前にトレーニングされた言語モデルの助けを借りて、セマンティック知識を表示されたクラスから表示されていないクラスに転送します。単純ですが、ピクセルレベルのZS3の定式化は、画像とテキストのペアで事前にトレーニングされていることが多く、現在、視覚タスクの可能性が非常に高い視覚言語モデルを統合する機能が制限されていることを示しています。人間がセグメントレベルのセマンティックラベリングを実行することが多いという観察に触発されて、ZS3を2つのサブタスクに分離することを提案します：1）ピクセルをセグメントにグループ化するクラスにとらわれないグループ化タスク2）セグメントのゼロショット分類タスク。前者のサブタスクはカテゴリ情報を含まず、見えないクラスのグループピクセルに直接転送できます。後者のサブタスクはセグメントレベルで実行され、ZS3の画像とテキストのペア（CLIPなど）で事前にトレーニングされた大規模な視覚言語モデルを活用する自然な方法を提供します。デカップリングの定式化に基づいて、ZegFormerと呼ばれるシンプルで効果的なゼロショットセマンティックセグメンテーションモデルを提案します。これは、ZS3標準ベンチマークの以前の方法を大幅に上回ります。たとえば、PASCAL VOCで35ポイント、COCO-Stuffinで3ポイントです。見えないクラスのmIoU。コードはhttps://github.com/dingjiansw101/ZegFormerでリリースされます。
--------------------------------------------------


## Homography Decomposition Networks for Planar Object Tracking
平面オブジェクト追跡のためのホモグラフィ分解ネットワーク 

- http://arxiv.org/pdf/2112.07909v1
### summary
平面オブジェクトトラッキングは、ロボティクス、ビジュアルサーボ、ビジュアルSLAMなどのAIアプリケーションで重要な役割を果たします。以前のplanartrackerはほとんどのシナリオでうまく機能しますが、2つの連続するフレーム間の急速な動きと大きな変換のため、それでも困難な作業です。この問題の背後にある本質的な理由は、ホモグラフィパラメータ空間の探索範囲が大きくなると、そのような非線形システムの条件数が不安定に変化することです。この目的のために、ホモグラフィ変換を2つのグループに分解することにより、条件数を大幅に削減および安定化する新しいHomographyDecomposition Networks〜（HDN）アプローチを提案します。具体的には、類似性変換推定量は、深い畳み込み同変ネットワークによって最初のグループをロバストに予測するように設計されています。信頼度の高いスケールと回転の推定を利用して、単純な回帰モデルによって残差変換を推定します。さらに、提案されたエンドツーエンドネットワークは、半教師ありの方法でトレーニングされます。広範な実験により、提案されたアプローチは、挑戦的なPOT、UCSB、およびPOICデータセットで、最先端の平面追跡方法よりも大幅に優れていることが示されています。
--------------------------------------------------


## Robust Depth Completion with Uncertainty-Driven Loss Functions
不確実性に起因する損失関数を使用したロバストな深さの完了 

- http://arxiv.org/pdf/2112.07895v1
### summary
スパースLiDARスキャンから高密度深度画像を復元することは困難な作業です。スパースから高密度の深度補完のためのカラーガイド方式の人気にもかかわらず、彼らは最適化中にピクセルを同等に扱い、スパース深度マップの不均一な分布特性と合成されたグラウンドトゥルース。この作業では、不確実性に基づく損失関数を導入して、深度完了のロバスト性を向上させ、深度完了の不確実性を処理します。具体的には、ジェフリーズ事前分布を使用して、ロバストな深度補完のための明示的な不確実性の定式化を提案します。パラメトリックな不確実性に起因する損失が導入され、ノイズの多いデータや欠落したデータに対してロバストな新しい損失関数に変換されます。一方、深度マップと不確実性マップを同時に予測できるマルチスケールジョイント予測モデルを提案します。推定された不確実性マップは、不確実性の高いピクセルの適応予測を実行するためにも使用され、完了結果を改良するための残差マップにつながります。私たちの方法は、KITTIDepth Completion Benchmarkでテストされ、MAE、IMAE、およびIRMSEメトリックの観点から最先端の堅牢性パフォーマンスを達成しました。
--------------------------------------------------


## Does a Face Mask Protect my Privacy?: Deep Learning to Predict Protected Attributes from Masked Face Images
フェイスマスクは私のプライバシーを保護しますか？：マスクされた顔の画像から保護された属性を予測するためのディープラーニング 

- http://arxiv.org/pdf/2112.07879v1
### summary
非接触で効率的なシステムは、COVID-19パンデミックとの闘いにおける予防方法を提唱するために迅速に実装されています。このようなシステムにはプラスのメリットがありますが、ユーザーのプライバシーを侵害することで悪用される可能性があります。この作業では、マスクされた顔画像を使用してプライバシーに敏感なソフトバイオメトリクスを予測することにより、顔バイオメトリクスシステムのプライバシー侵襲性を分析します。 ResNet-50アーキテクチャに基づいて20,003の合成マスク画像を使用してCNNをトレーニングおよび適用し、プライバシーの侵害性を測定します。人々の間でマスクを着用することのプライバシーの利点の一般的な信念にもかかわらず、マスクを着用した場合のプライバシー侵害性に有意差がないことを示しています。私たちの実験では、マスクされた顔の画像から性別（94.7％）、人種（83.1％）、年齢（MAE6.21およびRMSE8.33）を正確に予測することができました。私たちが提案するアプローチは、プライバシーに敏感な情報を利用する人工知能システムのプライバシー侵害性を評価するためのベースラインユーティリティとして役立ちます。私たちは、研究コミュニティによる再現性と幅広い使用のために、すべての貢献をオープンソース化します。
--------------------------------------------------


## Gaze Estimation with Eye Region Segmentation and Self-Supervised Multistream Learning
眼球領域セグメンテーションと自己監視マルチストリーム学習による視線推定 

- http://arxiv.org/pdf/2112.07878v1
### summary
視線推定のためのロバストな目の表現を学習する新しいマルチストリームネットワークを提示します。まず、シミュレータを使用して、目に見える眼球と虹彩の詳細を示す目の領域マスクを含む合成データセットを作成します。次に、U-Netタイプのモデルを使用して目の領域のセグメンテーションを実行します。このモデルは、後で実際の目の画像の目の領域のマスクを生成するために使用します。次に、一般化された目の表現を学習するために、自己監視対照学習を使用して、実領域で目の画像エンコーダーを事前トレーニングします。最後に、この事前トレーニングされた目のエンコーダーは、目に見える眼球領域と虹彩用の2つの追加のエンコーダーとともに、マルチストリームフレームワークで並行して使用され、実世界の画像から視線推定のための顕著な特徴を抽出します。 2つの異なる評価設定でEYEDIAPデータセットに対するメソッドのパフォーマンスを示し、このデータセットの既存のすべてのベンチマークを上回る最先端の結果を達成します。また、トレーニングに使用されるさまざまな量のラベル付きデータに関して、自己監視型ネットワークの堅牢性を検証するための追加の実験を実施します。
--------------------------------------------------


## Mining Minority-class Examples With Uncertainty Estimates
不確実性の見積もりを伴うマイノリティクラスの例のマイニング 

- http://arxiv.org/pdf/2112.07835v1
### summary
現実の世界では、オブジェクトの発生頻度は自然に歪んでロングテールクラス分布を形成するため、統計的にまれなクラスのパフォーマンスが低下します。有望な解決策は、トレーニングデータセットのバランスをとるためにテールクラスの例をマイニングすることです。ただし、テールクラスの例をマイニングすることは非常に困難な作業です。たとえば、他の点では成功している不確実性ベースのマイニングアプローチのほとんどは、データの歪度に起因するクラス確率の歪みのために苦労しています。この作業では、これらの課題を克服するための効果的でありながらシンプルなアプローチを提案します。私たちのフレームワークは、抑制されたテールクラスのアクティブ化を強化し、その後、1クラスのデータ中心のアプローチを使用してテールクラスの例を効果的に識別します。 2つのコンピュータービジョンタスクにまたがる3つのデータセットに対して、フレームワークの徹底的な評価を実行します。マイノリティクラスのマイニングと微調整されたモデルのパフォーマンスの大幅な改善は、提案されたソリューションの価値を強く裏付けています。
--------------------------------------------------


## Value Retrieval with Arbitrary Queries for Form-like Documents
フォームのようなドキュメントの任意のクエリによる値の取得 

- http://arxiv.org/pdf/2112.07820v1
### summary
フォームを処理する人的労力を削減するために、フォームのようなドキュメントに対して任意のクエリを使用した値検索を提案します。フィールドアイテムの固定セットのみに対応する以前の方法とは異なり、この方法では、フォームのレイアウトとセマンティクスの理解に基づいて任意のクエリのターゲット値を予測します。モデルのパフォーマンスをさらに向上させるために、単純なドキュメント言語モデリング（simpleDLM）戦略を提案して改善します。大規模モデルの事前トレーニングに関するドキュメントの理解。実験結果は、私たちの方法が私たちのベースラインを大幅に上回り、simpleDLMが最先端の事前トレーニング方法と比較して約17 \％F1スコアでパフォーマンスオンバリュー検索をさらに改善することを示しています。コードは公開されます。
--------------------------------------------------


## Weed Recognition using Deep Learning Techniques on Class-imbalanced Imagery
クラス不均衡画像での深層学習技術を使用した雑草認識 

- http://arxiv.org/pdf/2112.07819v1
### summary
ほとんどの雑草種は、高価値作物に必要な栄養素を奪い合うことにより、農業生産性に悪影響を与える可能性があります。手動による除草は、広い作付面積では実用的ではありません。農作物の自動雑草管理システムを開発するために多くの研究が行われてきました。このプロセスでの主要なタスクの1つは、画像から雑草を認識することです。ただし、雑草の認識は困難な作業です。雑草と作物は似たような色、質感、形をしているため、画像を記録する際の画像条件、地理的条件、気象条件によってさらに悪化する可能性があります。高度な機械学習技術を使用して、画像から雑草を認識することができます。この論文では、5つの最先端の深部神経ネットワーク、すなわちVGG16、ResNet-50、Inception-V3、Inception-ResNet-v2、およびMobileNetV2を調査し、雑草認識のためのそれらの性能を評価しました。いくつかの実験設定と複数のデータセットの組み合わせを使用しました。特に、いくつかの小さなデータセットを組み合わせ、データ拡張によってクラスの不均衡を緩和し、このデータセットを使用してディープニューラルネットワークをベンチマークすることにより、大きな雑草作物データセットを構築しました。特徴を抽出するために事前にトレーニングされた重みを保持し、作物と雑草のデータセットの画像を使用してそれらを微調整することにより、転移学習手法の使用を調査しました。 VGG16は小規模なデータセットで他のネットワークよりも優れたパフォーマンスを示し、ResNet-50は大規模な結合データセットで他のディープネットワークよりも優れたパフォーマンスを示しました。
--------------------------------------------------


## Image Segmentation with Homotopy Warping
ホモトピーワーピングによる画像セグメンテーション 

- http://arxiv.org/pdf/2112.07812v1
### summary
ピクセルごとの精度に加えて、トポロジーの正確さは、衛星画像や生物医学画像などの微細構造を持つ画像のセグメンテーションにも重要です。この論文では、デジタルトポロジーの理論を活用することにより、トポロジーにとって重要な画像内の場所を特定します。これらの重要な場所に焦点を当てることにより、トポロジーの精度を高めるために、トレインディープ画像セグメンテーションネットワークに新しいホモトピーワーピング損失を提案します。これらのトポロジー的に重要な場所を効率的に識別するために、距離変換を利用する新しいアルゴリズムを提案します。提案されたアルゴリズムと損失関数は、2Dと3Dの両方の設定で異なるトポロジー構造に自然に一般化されます。提案された損失関数は、トポロジー認識メトリックの観点からディープネットがより良いパフォーマンスを達成するのに役立ち、最先端のトポロジー保存セグメンテーション方法を上回ります。
--------------------------------------------------


